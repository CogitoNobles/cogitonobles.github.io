---
layout: post
title: "Putting the AI in Bias"
author: "Emma Edgar"
feature_image: /cogito-xiii-pictures/edgar.jpeg
categories:
  - Cogito-XIII
  - Articles
---
You are standing in line at J.P Licks, looking at the various flavors of ice cream, shielded behind the glass partition by the cash register. When it is finally your turn, you ask for a peanut butter sundae. Over the course of the year, you go another five times, ordering frappes, pies, and cakes. 

Now, imagine you wanted to create an artificial intelligence (AI) system that could predict what someone would order on their next visit to any ice cream shop based on their past purchases. How would you do that, given the wide selection of possible products for purchase at numerous different ice cream chains? What if one person prefers the cookie dough of one place to another? 

Taking it even further, what if a business wanted to expand this system to identify new customers and determine what they would order as soon as they walked through the door?

Many companies are trying to predict and influence our decisions, like in our ice cream example, but in situations that have more life altering consequences. For example, these algorithms could determine whether or not you receive a loan, if you get a follow up interview with an employer, and whether a facial recognition system can identify you. What if there are false negatives (failing to identify a known person) and false positives (allowing an impostor)? All of these methods of prediction and decision making can influence where we live, our income, socioeconomic status, and the opportunities and resources we receive. 

Another major component of the composition of algorithms is deciding between fairness and the accuracy of a predictive model. Machine learning models are trained to reproduce historical data. Consequently, if there are biases in the data, they will be reproduced endlessly unless there is intervention by the model builders. Artificial intelligence learning systems do not learn from the past; they learn how to make the future look like the past. So, engineers must learn how to maintain fairness by carefully evaluating the data they supply the system.

So, does the public have the right to know how these systems are developed and decide whether or not they should be used?

The information to make such predictions is through the lens of our flawed human biases (whether related to race, gender, sexuality, ethnicity, educational background etc.) that are not often explicitly mentioned in AI development. Also, there is a distinction between the user(s) and subject(s) of these systems. Going back to our ice cream example, the customer would be the subject of the algorithm and the owner of the ice cream shop would be the user. The owner may have to choose between different goals for the system: to make the most amount of profit or to have the highest possible rate of customer satisfaction? Is it to determine what ice cream flavor is the most popular or to figure out how to revive an old flavor that has become less popular?

Many might assume that bias is impossible, since AI systems use the same set of facts about each individual. This is called procedural fairness. Yet, this line of thinking does not  acknowledge historical contexts preceding technology that continue to impact people to this day. Generational wealth, any kind of monetary asset passed down from one generation to another, is especially important when it comes to equality. One of the easiest ways to build equity is through homeownership, giving someone more leverage to grow more wealth. CNBC notes that the racial wealth gap is larger today than it was in 1960 due to the legacy of redlining, where banks imposed more obstacles for loans in Black communities along with higher interest rates, fewer approvals, and categorizing them as high risk customers. Communities of color also tend to pay higher taxes, and not nearly enough of these funds go directly to help improve services like grocery stores, banks, parks, and community centers. Many banks now process loan applications through AI systems, trained on historic data. Due to the legacy of redlining, unless special care is taken, these systems will continue the same pattern.

Is it really fair to deny someone a mortgage simply because of previously inherited debt due to prejudices, and who may have not had the best educational opportunities to attain a job that would make paying such a loan possible? How are they going to be able to build generational wealth if they keep getting denied, unlike those who were accepted and now have it? Is it fair to punish someone for this when they live in a world with systems that are made for them to fail? Does that not in itself perpetuate the cycle once more?

Consider the case of college admissions. Student applications may be reviewed and prioritized by an AI system. Inevitably, a substantial component of the resultant model is going to be scores on standardized tests, since these will be so easy to extract and compare (much simpler than assessing an essay). However, scores in such tests can be affected by tutoring. According to the Washington Post, a study from the National Association for College Admission Counseling concluded in 2009 that people who were coached by test prep firms, some costing more than $1,000, got SAT scores that were sometimes only 30 points higher than that of someone who was not tutored privately. However, it was still enough to make a difference in acceptance decisions. This is not to say that someone cannot do well without tutoring but that their score relative to those who have had the tutoring suffer.

The demographics of the people who build the models matters as well. Many decisions are made as an AI model is built, and these decisions will be affected by the implicit biases of the model builders. According to Zippia.com, this workforce is more (78%) male than the US population, and people identifying as black are substantially under-represented (just under 5%, when the fraction in the US population is over 12%). Even with a conscious effort to avoid implicit biases, such an imbalance will inevitably lead to oversights in model construction, and unfair models. 

The people who create these systems should be responsible for the effects they have on its users, with or without consent. Many of them have no legal responsibility for the effects a system may have. Regulations on AI technology are not prioritized, even as they become more prominent in our lives. The wealth of the technology industry gives it immense lobbying power, helping it to control the narrative surrounding AI bias. AI systems affect what our lives in myriad ways, ranging from the songs we hear to whether we are offered employment, yet it is hard to find out how these systems are developed and with what ulterior goal. A music recommendation is designed to persuade you to buy a copy or to listen to the prepended advert; the actual song is secondary.

To create and refine more equitable and fair systems in our world, we must hold ourselves and technology companies accountable when using AI by inviting a wider audience to participate in the design process. We cannot tolerate the current one-sided relationship where tech companies call all the shots and leave the public in the dark. If we are to expect any improvements in equity between people, we must remember how these systems are made to reproduce results from the past, the exact opposite of our intent. Machine learning systems need to be constantly analyzed and reprogrammed to better measure and understand the subject matter at hand.
